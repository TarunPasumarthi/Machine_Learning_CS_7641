{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T17:17:25.168179Z",
     "start_time": "2019-06-20T17:17:24.986130Z"
    },
    "colab_type": "text",
    "id": "Hjp__3bRh42K",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Spring 2020 CX4641/CS7641 Homework 4\n",
    "\n",
    "## Instructor: Dr. Mahdi Roozbahani\n",
    "\n",
    "## Deadline: Apr 21st, 11:59 pm\n",
    "\n",
    "* You are allowed to resubmit your AS4 by 11:59pm on April 28th, 2020 without any penalty.\n",
    "\n",
    "* Homework submission ONLY in .ipynb format.\n",
    "\n",
    "* Discussion is encouraged on Piazza, but each student must write their own answers and explicitly mention any collaborators.\n",
    "\n",
    "* Throughout the ipython notebook, we use attribute and feature interchangeably.\n",
    "\n",
    "* Graduate students are required to answer the compulsory questions and the questions that are bonus for undergraduate students. There is one question that is bonus for all students.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7U0WVt07tGRv"
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0Ui6T2as9iI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from math import log2, sqrt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HdkkppO-pNEn"
   },
   "source": [
    "# Part 1: Utility Functions (25 pts)\n",
    "\n",
    "## Part 1.1: Evaluation Utility Functions\n",
    "\n",
    "Here, we ask you to develop a few functions that will be the main building blocks of your decision tree and random forest algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7e_A6GBe11nA"
   },
   "source": [
    "### Entropy and information gain [10pts]\n",
    "\n",
    "First, we define and implement a function that computes entropy of the data.\\\n",
    "Then use this entropy function to compute the information gain for the partitioned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVcXl9-D2DMW"
   },
   "outputs": [],
   "source": [
    "def entropy(class_y):\n",
    "    \"\"\" \n",
    "    Input: \n",
    "        - class_y: list of class labels (0's and 1's)\n",
    "    \n",
    "    TODO: Compute the entropy for a list of classes\n",
    "    Example: entropy([0,0,0,1,1,1,1,1]) = 0.9544\n",
    "    \"\"\"\n",
    "    counts=Counter(class_y)\n",
    "    entropy=0\n",
    "    for c in counts:\n",
    "        p=counts[c]/len(class_y)\n",
    "        entropy-= (p*np.log2(p))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wp7wEfZ82Owg"
   },
   "outputs": [],
   "source": [
    "def information_gain(previous_y, current_y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - previous_y : the distribution of original labels (0's and 1's)\n",
    "        - current_y  : the distribution of labels after splitting based on a particular\n",
    "                     split attribute and split value\n",
    "    \n",
    "    TODO: Compute and return the information gain from partitioning the previous_y labels into the current_y labels.\n",
    "    \n",
    "    Reference: https://mahdi-roozbahani.github.io/CS46417641-spring2020/course/17-decision-tree.pdf\n",
    "\n",
    "    Example: previous_y = [0,0,0,1,1,1], current_y = [[0,0], [1,1,1,0]], info_gain = 0.4591\n",
    "    \"\"\" \n",
    "        \n",
    "    prev_ent=entropy(previous_y)\n",
    "    curr_ent=0\n",
    "    \n",
    "    for split in current_y:\n",
    "        if(len(split)==0):\n",
    "            continue\n",
    "        w= len(split)/len(previous_y)\n",
    "        curr_ent+= w*entropy(split)\n",
    "        \n",
    "    ig= prev_ent- curr_ent\n",
    "    return ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z-nbf6IJ2fkS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.954434002924965\n",
      "0.4591479170272448\n"
     ]
    }
   ],
   "source": [
    "# TEST CASE\n",
    "test_class_y = [0,0,0,1,1,1,1,1]\n",
    "print(entropy(test_class_y))\n",
    "\n",
    "previous_y = [0,0,0,1,1,1]\n",
    "current_y = [[0,0], [1,1,1,0]] \n",
    "print(information_gain(previous_y, current_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HdkkppO-pNEn"
   },
   "source": [
    "## Part 1.2: Splitting Utility Functions\n",
    "\n",
    "Building a decision tree requires us to evaluate the best feature and value to split a node on. Now we will implement functions that help us determine these splits for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9mBJIZFd2lx4"
   },
   "source": [
    "#### (1) partition_classes: [5pts]\n",
    "\n",
    "One of the basic operations is to split a tree on one attribute (features) with a specific value for that attribute.\n",
    "\n",
    "In partition_classes(), we split the data (X) and labels (y) based on the split feature and value - BINARY SPLIT.\n",
    "\n",
    "You will have to first check if the split attribute is numerical or categorical. If the split attribute is numeric, split_val should be a numerical value. For example, your split_val should go over all the values of attributes. If the split attribute is categorical, split_val should include all the categories one by one.   \n",
    "    \n",
    "You can perform the partition in the following way:\n",
    "   - Numeric Split Attribute:\n",
    "   \n",
    "       Split the data X into two lists(X_left and X_right) where the first list has all\n",
    "       the rows where the split attribute is less than or equal to the split value, and the \n",
    "       second list has all the rows where the split attribute is greater than the split \n",
    "       value. Also create two lists(y_left and y_right) with the corresponding y labels.\n",
    "    \n",
    "   - Categorical Split Attribute:\n",
    "   \n",
    "       Split the data X into two lists(X_left and X_right) where the first list has all \n",
    "       the rows where the split attribute is equal to the split value, and the second list\n",
    "       has all the rows where the split attribute is not equal to the split value.\n",
    "       Also create two lists(y_left and y_right) with the corresponding y labels.\n",
    "\n",
    "\n",
    "Hint: You could find out if the feature is categorical by checking if it is the instance of 'str'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-Ch02qB2oJm"
   },
   "outputs": [],
   "source": [
    "def partition_classes(X, y, split_attribute, split_val):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X               : (N,D) list containing all data attributes\n",
    "    - y               : a list of labels\n",
    "    - split_attribute : column index of the attribute to split on\n",
    "    - split_val       : either a numerical or categorical value to divide the split_attribute\n",
    "    \n",
    "    TODO: Partition the data(X) and labels(y) based on the split value - BINARY SPLIT.\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    X = [[3, 'aa', 10],                 y = [1,\n",
    "         [1, 'bb', 22],                      1,\n",
    "         [2, 'cc', 28],                      0,\n",
    "         [5, 'bb', 32],                      0,\n",
    "         [4, 'cc', 32]]                      1]\n",
    "    \n",
    "    Here, columns 0 and 2 represent numeric attributes, while column 1 is a categorical attribute.\n",
    "    \n",
    "    Consider the case where we call the function with split_attribute = 0 (the index of attribute) and split_val = 3 (the value of attribute).\n",
    "    Then we divide X into two lists - X_left, where column 0 is <= 3 and X_right, where column 0 is > 3.\n",
    "    \n",
    "    X_left = [[3, 'aa', 10],                 y_left = [1,\n",
    "              [1, 'bb', 22],                           1,\n",
    "              [2, 'cc', 28]]                           0]\n",
    "              \n",
    "    X_right = [[5, 'bb', 32],                y_right = [0,\n",
    "               [4, 'cc', 32]]                           1]\n",
    "\n",
    "    Consider another case where we call the function with split_attribute = 1 and split_val = 'bb'\n",
    "    Then we divide X into two lists, one where column 1 is 'bb', and the other where it is not 'bb'.\n",
    "        \n",
    "    X_left = [[1, 'bb', 22],                 y_left = [1,\n",
    "              [5, 'bb', 32]]                           0]\n",
    "              \n",
    "    X_right = [[3, 'aa', 10],                y_right = [1,\n",
    "               [2, 'cc', 28],                           0,\n",
    "               [4, 'cc', 32]]                           1]\n",
    "               \n",
    "               \n",
    "    Return in this order: X_left, X_right, y_left, y_right       \n",
    "    \"\"\"   \n",
    "    \n",
    "    X_left= []\n",
    "    X_right= []\n",
    "    y_left= []\n",
    "    y_right= []\n",
    "    \n",
    "    Xy=np.hstack((X,np.array([y]).T))\n",
    "    \n",
    "    if(isinstance(split_val, str)):\n",
    "        Xy_left=Xy[np.where(Xy[:,split_attribute] == split_val)]\n",
    "        Xy_right=Xy[np.where(Xy[:,split_attribute] != split_val)]\n",
    "    else:\n",
    "        Xy_left=Xy[np.where(Xy[:,split_attribute] <= split_val)]\n",
    "        Xy_right=Xy[np.where(Xy[:,split_attribute] > split_val)]\n",
    "    \n",
    "    \"\"\"\n",
    "    for i, point in enumerate(X):\n",
    "        if(isinstance(split_val, str)):\n",
    "            if(point[split_attribute]==split_val):\n",
    "                X_left.append(point)\n",
    "                y_left.append(y[i])\n",
    "            else:\n",
    "                X_right.append(point)\n",
    "                y_right.append(y[i])\n",
    "        else:\n",
    "            temp=int(point[split_attribute])\n",
    "            if(temp<=split_val):\n",
    "                X_left.append(point)\n",
    "                y_left.append(y[i])\n",
    "            else:\n",
    "                X_right.append(point)\n",
    "                y_right.append(y[i])\n",
    "    \"\"\"\n",
    "    \n",
    "    X_left= Xy_left[:,:-1]\n",
    "    X_right= Xy_right[:,:-1]\n",
    "    y_left= Xy_left[:,-1]\n",
    "    y_right= Xy_right[:,-1]\n",
    "    return X_left, X_right, y_left, y_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GpsToj_T2x42"
   },
   "source": [
    "#### (2) find_best_split [5pts]\n",
    "\n",
    "Given the data and labels, we need to find the order of splitting features, which is also the importance of the feature. For each attribute (feature), we need to calculate its optimal split value along with the corresponding information gain and then compare with all the features to find the optimal attribute to split.\n",
    "\n",
    "First, we specify an attribute. After computing the corresponding information gain of each value at this attribute list, we can get the optimal split value, which has the maximum information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nG3Def4L20re"
   },
   "outputs": [],
   "source": [
    "def find_best_split(X, y, split_attribute):\n",
    "    \"\"\"Inputs:\n",
    "        - X               : (N,D) list containing all data attributes\n",
    "        - y               : a list array of labels\n",
    "        - split_attribute : Column of X on which to split\n",
    "    \n",
    "    TODO: Compute and return the optimal split value for a given attribute, along with the corresponding information gain\n",
    "    \n",
    "    Note: You will need the functions information_gain and partition_classes to write this function\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "        X = [[3, 'aa', 10],                 y = [1,\n",
    "             [1, 'bb', 22],                      1,\n",
    "             [2, 'cc', 28],                      0,\n",
    "             [5, 'bb', 32],                      0,\n",
    "             [4, 'cc', 32]]                      1]\n",
    "    \n",
    "        split_attribute = 0\n",
    "        \n",
    "        Starting entropy: 0.971\n",
    "        \n",
    "        Calculate information gain at splits:\n",
    "           split_val = 1  -->  info_gain = 0.17\n",
    "           split_val = 2  -->  info_gain = 0.01997\n",
    "           split_val = 3  -->  info_gain = 0.01997\n",
    "           split_val = 4  -->  info_gain = 0.32\n",
    "           split_val = 5  -->  info_gain = 0\n",
    "        \n",
    "       best_split_val = 4; info_gain = .32; \n",
    "    \"\"\"\n",
    "    sv_vals=list(set(X[:,split_attribute]))\n",
    "    best_split_val=\"\"\n",
    "    best_ig=0\n",
    "    for sv in sv_vals:\n",
    "        X_left, X_right, y_left, y_right=partition_classes(X, y, split_attribute, sv)\n",
    "        ig= information_gain(y, [y_left,y_right])\n",
    "        if(ig>best_ig):\n",
    "            best_ig=ig\n",
    "            best_split_val=sv\n",
    "    return best_split_val,best_ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8eM_fLu3GN9"
   },
   "source": [
    "#### (3)  find_best_feature [5pts]\n",
    "\n",
    "Based on the above functions, we can find the most important feature that we will split first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "253k0w6Y3ISy"
   },
   "outputs": [],
   "source": [
    "def find_best_feature(X, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - X: (N,D) list containing all data attributes\n",
    "        - y : a list of labels\n",
    "    \n",
    "    TODO: Compute and return the optimal attribute to split on and optimal splitting value\n",
    "    \n",
    "    Note: If two features tie, choose one of them at random\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "        X = [[3, 'aa', 10],                 y = [1,\n",
    "             [1, 'bb', 22],                      1,\n",
    "             [2, 'cc', 28],                      0,\n",
    "             [5, 'bb', 32],                      0,\n",
    "             [4, 'cc', 32]]                      1]\n",
    "    \n",
    "        split_attribute = 0\n",
    "        \n",
    "        Starting entropy: 0.971\n",
    "        \n",
    "        Calculate information gain at splits:\n",
    "           feature 0:  -->  info_gain = 0.32\n",
    "           feature 1:  -->  info_gain = 0.17\n",
    "           feature 2:  -->  info_gain = 0.4199\n",
    "        \n",
    "       best_split_feature: 2 best_split_val: 22\n",
    "    \"\"\"\n",
    "    \n",
    "    #  Delete this line when you implement the function\n",
    "    best_att=0\n",
    "    best_sv=\"\"\n",
    "    best_ig=0\n",
    "    for att in range(X.shape[1]):\n",
    "        sv,ig= find_best_split(X, y, att)\n",
    "        if(ig>best_ig):\n",
    "            best_ig=ig\n",
    "            best_sv=sv\n",
    "            best_att=att\n",
    "    return best_att, best_sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O0IBRUsW3P-7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[3, 'aa', 10],\n",
      "       [1, 'bb', 22],\n",
      "       [2, 'cc', 28]], dtype=object), array([[5, 'bb', 32],\n",
      "       [4, 'cc', 32]], dtype=object), array([1, 1, 0], dtype=object), array([0, 1], dtype=object))\n",
      "(array([[1, 'bb', 22],\n",
      "       [5, 'bb', 32]], dtype=object), array([[3, 'aa', 10],\n",
      "       [2, 'cc', 28],\n",
      "       [4, 'cc', 32]], dtype=object), array([1, 0], dtype=object), array([1, 0, 1], dtype=object))\n",
      "best_split_val: 4 info_gain: 0.3219280948873623\n",
      "best_split_feature: 2 best_split_val: 22\n"
     ]
    }
   ],
   "source": [
    "# TEST CASE\n",
    "test_X = [[3, 'aa', 10],[1, 'bb', 22],[2, 'cc', 28],[5, 'bb', 32],[4, 'cc', 32]]\n",
    "test_y = [1,1,0,0,1]\n",
    "test_X, test_y = np.array(test_X, dtype='object'), np.array(test_y)\n",
    "print(partition_classes(test_X, test_y, 0, 3))\n",
    "print(partition_classes(test_X, test_y, 1, 'bb'))\n",
    "\n",
    "split_attribute = 0\n",
    "best_split_val, info_gain = find_best_split(test_X, test_y, split_attribute)\n",
    "print(\"best_split_val:\", best_split_val, \"info_gain:\", info_gain)\n",
    "\n",
    "best_feature, best_split_val = find_best_feature(test_X, test_y)\n",
    "print(\"best_split_feature:\", best_feature, \"best_split_val:\", best_split_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ayv_tqnyxMb0"
   },
   "source": [
    "# Part 2: Decision Tree (20 pts)\n",
    "## Please read the following instructions carefully before you dive into coding\n",
    "\n",
    "In this part, you will implement your own ID3 decision tree class and make it work on training and test set (ID3 is the name of the algorithm to construct the decision tree).\n",
    "\n",
    "You may use a recursive way to construct the tree and make use of helper functions in Part1. \n",
    "\n",
    "Please keep in mind that we use information gain to find the best feature and value to split the data for ID3 tree.\n",
    "\n",
    "To save your training time, we have added a ```max_depth``` parameter to control the maximum depth of the tree. You may adjust its value to pre-prune the tree. If set to None, it has no control of depth.\n",
    "\n",
    "You need to have a stop condition for splitting. The stopping condtion is reached when one of the two following conditions are met:\n",
    "1. If all data points in that node have the same label\n",
    "2. If the current node is at the maximum depth. In this case, you may assign the mode of the labels as the class label\n",
    "\n",
    "The MyDecisionTree class should have some member variables. We highly encourage you to use a [dict data structure](https://www.w3schools.com/python/python_dictionaries.asp) in Python to store the tree information. For leaves nodes, this dict may have just one element representing the class label. For non-leaves node, the list should at least store the feature and value to split, and references to its left and right child.\n",
    "\n",
    "An example of the dict that you may use for non-leaf nodes:\n",
    "\n",
    "<pre> node = {\n",
    "            'isLeaf': False,\n",
    "            'split_attribute': split_attribute,\n",
    "            'split_value': split_val,\n",
    "            'is_categorical': is_categorical,\n",
    "            'leftTree': leftTree,\n",
    "            'rightTree': rightTree\n",
    "        };\n",
    "</pre>\n",
    "\n",
    "In the above example, the leftTree and rightTree are instances of MyDecisonTree itself.\n",
    "\n",
    "### If you use different ways to represent and store the information, please include clear comments or documentations with your code. If your result is not correct, partial credits can only be awarded if we are able to understand your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2Cwef24xgtT"
   },
   "outputs": [],
   "source": [
    "class MyDecisionTree(object):\n",
    "    def __init__(self, max_depth=None):\n",
    "        \"\"\"\n",
    "        TODO: Initializing the tree as an empty dictionary, as preferred.\n",
    "        \n",
    "        For example: self.tree = {}\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        max_depth: maximum depth of the tree including the root node.\n",
    "        \"\"\"\n",
    "        \n",
    "        # label -1 indicates not a leaf\n",
    "        self.tree = {\n",
    "            'label': -1,\n",
    "            'split_attribute': None,\n",
    "            'split_value': None,\n",
    "            'leftTree': None,\n",
    "            'rightTree': None\n",
    "        }\n",
    "        self.max_depth=max_depth\n",
    "        if(max_depth==None):\n",
    "            self.max_depth=np.inf\n",
    "\n",
    "    def fit(self, X, y, depth):\n",
    "        \"\"\"\n",
    "        TODO: Train the decision tree (self.tree) using the the sample X and labels y.\n",
    "        \n",
    "        NOTE: You will have to make use of the utility functions to train the tree.\n",
    "        One possible way of implementing the tree: Each node in self.tree could be in the form of a dictionary:\n",
    "        https://docs.python.org/2/library/stdtypes.html#mapping-types-dict\n",
    "        \n",
    "        For example, a non-leaf node with two children can have a 'left' key and  a  'right' key. \n",
    "        You can add more keys which might help in classification (eg. split attribute and split value)\n",
    "        \n",
    "        \n",
    "        While fitting a tree to the data, you will need to check to see if the node is a leaf node(\n",
    "        based on the stopping condition explained above) or not. \n",
    "        If it is not a leaf node, find the best feature and attribute split:\n",
    "        X_left, X_right, y_left, y_right, for the data to build the left and\n",
    "        the right subtrees.\n",
    "        \n",
    "        Remember for building the left subtree, pass only X_left and y_left and for the right subtree,\n",
    "        pass only X_right and y_right.\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        X: N*D matrix corresponding to the data points\n",
    "        Y: N*1 array corresponding to the labels of the data points\n",
    "        depth: depth of node of the tree\n",
    "        \n",
    "        \"\"\"\n",
    "        if(depth==self.max_depth or len(set(y))==1):\n",
    "            self.tree[\"label\"]=stats.mode(y)[0][0]\n",
    "            return\n",
    "            \n",
    "        best_att, best_sv= find_best_feature(X, y)\n",
    "        X_left, X_right, y_left, y_right= partition_classes(X, y, best_att, best_sv)\n",
    "        self.tree[\"split_attribute\"]=best_att\n",
    "        self.tree[\"split_value\"]=best_sv\n",
    "        self.tree[\"leftTree\"]= MyDecisionTree(self.max_depth)\n",
    "        self.tree[\"rightTree\"]=MyDecisionTree(self.max_depth)\n",
    "        self.tree[\"leftTree\"].fit(X_left,y_left,depth+1)\n",
    "        self.tree[\"rightTree\"].fit(X_right,y_right,depth+1)\n",
    "           \n",
    "       \n",
    "    def predict(self, record):\n",
    "        \"\"\"\n",
    "        TODO: classify a sample in test data set using self.tree and return the predicted label\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        record: D*1, a single data point that should be classified\n",
    "        \n",
    "        Returns: True if the predicted class label is 1, False otherwise      \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        if(self.tree[\"label\"]!=-1):\n",
    "            return self.tree[\"label\"]\n",
    "        att=self.tree[\"split_attribute\"]\n",
    "        sv=self.tree[\"split_value\"]\n",
    "        if(isinstance(sv, str)):\n",
    "            if(record[att]==sv):\n",
    "                return self.tree[\"leftTree\"].predict(record)\n",
    "            else:\n",
    "                return self.tree[\"rightTree\"].predict(record)\n",
    "        else:\n",
    "            if(record[att]<=sv):\n",
    "                return self.tree[\"leftTree\"].predict(record)\n",
    "            else:\n",
    "                return self.tree[\"rightTree\"].predict(record)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0IvbriYy5yN"
   },
   "outputs": [],
   "source": [
    "# helper function. You don't have to modify it\n",
    "def DecisionTreeEvalution(dt,X,y, verbose=False):\n",
    "\n",
    "    # Make predictions\n",
    "    # For each test sample X, use our fitting dt classifer to predict\n",
    "    y_predicted = []\n",
    "    for record in X: \n",
    "        y_predicted.append(dt.predict(record))\n",
    "\n",
    "    # Comparing predicted and true labels\n",
    "    results = [prediction == truth for prediction, truth in zip(y_predicted, y)]\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True)) / float(len(results))\n",
    "    if verbose:\n",
    "        print(\"accuracy: %.4f\" % accuracy)\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Objective\n",
    "\n",
    "We are quant researchers in a new credit company, \"50K Credit\". We are tasked with the responsibility of coming up with a screening system that lets our employees know if an individual is eligible for the \"new and exciting\" credit card that our company is going to launch in July 2020.\n",
    "\n",
    "After much deliberation amongst all researchers, you come to a conclusion that any individual that earns more than $50K annually should be given the credit card.\n",
    "\n",
    "Our task is to use the decision tree algorithm to predict if an individual earns more than $50K annually and is therefore eligible for the new credit card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "\n",
    "The dataset that the company has collected has the following features:\n",
    "\n",
    "1. age : continuous variable\n",
    "2. state_sample\t: continuous variable\n",
    "3. education_num : continuous variable\n",
    "4. capital_gain : continuous variable\n",
    "5. capital_loss : continuous variable\n",
    "6. hours_per_week : continuous variable\n",
    "7. workclass : categorical variable\n",
    "8. education : categorical variable\n",
    "9. marital_status : categorical variable\n",
    "10. occupation : categorical variable\n",
    "11. relationship\t: categorical variable\n",
    "12. c1\t: categorical variable\n",
    "13. c2\t: categorical variable\n",
    "14. c3 : categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function. You don't have to modify it\n",
    "data_test = pd.read_csv(\"hw4_spring2020_data_test.csv\")\n",
    "data_valid = pd.read_csv(\"hw4_spring2020_data_valid.csv\")\n",
    "data_train = pd.read_csv(\"hw4_spring2020_data_train.csv\")\n",
    "\n",
    "categorical = ['workclass', 'education', 'marital_status', 'occupation', \n",
    "                   'relationship', 'c1', 'c2', 'c3']\n",
    "numerical = ['age', 'state_sample', 'education_num','capital_gain', 'capital_loss',\n",
    "                'hours_per_week']\n",
    " \n",
    "X_train = pd.concat([data_train[categorical], data_train[numerical]], axis=1)\n",
    "y_train = data_train['eligible']\n",
    "X_test = pd.concat([data_test[categorical], data_test[numerical]], axis=1)\n",
    "y_test = data_test['eligible']\n",
    "X_train, y_train, X_test, y_test = np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test)\n",
    "\n",
    "X_valid = pd.concat([data_valid[categorical], data_valid[numerical]], axis=1)\n",
    "y_valid = data_valid['eligible']\n",
    "X_valid, y_valid = np.array(X_valid), np.array(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train and evaluate the performance of our decision tree on the test set. Note that it is trivially possible to achieve 75% accuracy because of the disribution of \"eligible\" candidates in the dataset. You should aim to get an accuracy of atleast 80% on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting the decision tree\n"
     ]
    }
   ],
   "source": [
    "# Initializing a decision tree.\n",
    "max_depth = 7\n",
    "dt = MyDecisionTree(max_depth)\n",
    "\n",
    "# Building a tree\n",
    "print(\"fitting the decision tree\")\n",
    "dt.fit(X_train, y_train, 0)\n",
    "\n",
    "# Evaluating the decision tree\n",
    "DecisionTreeEvalution(dt,X_test,y_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "# Part 3\n",
    "## Pruning [10 Pts] [Bonus for undergrads]\n",
    "\n",
    "In order to avoid overfitting, you can:\n",
    "1. Acquire more training data; \n",
    "2. Remove irrelevant attributes; \n",
    "3. Grow full tree, then post-prune; \n",
    "4. Ensemble learning. \n",
    "\n",
    "In this part, you are going to apply reduced error post-pruning to prune the fully grown tree in a bottom-up manner.\n",
    "The idea is basically about, starting at the leaves, each node is replaced with its most popular class. If the prediction accuracy is not affected then the change is kept. You may also try recursive function to apply the post-pruning. Please notice we use validation set to get the accuracy for each node during the pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTreeError(y):\n",
    "    # helper function for calculating the error of the entire subtree if converted to a leaf with majority class label.\n",
    "    # You don't have to modify it  \n",
    "    num_ones = np.sum(y)\n",
    "    num_zeros = len(y) - num_ones\n",
    "    return 1.0 - max(num_ones, num_zeros) / float(len(y))\n",
    "\n",
    "\n",
    "#  Define the post-pruning function\n",
    "def pruning(dt, X, y):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    1. Prune the full grown decision tress recursively in a bottom up manner.  \n",
    "    2. Classify examples in validation set.\n",
    "    3. For each node: \n",
    "    3.1 Sum errors over the entire subtree. You may want to use the helper function \"DecisionTreeEvalution\".\n",
    "    3.2 Calculate the error on same example if converted to a leaf with majority class label. \n",
    "    You may want to use the helper function \"DecisionTreeError\".\n",
    "    4. If error rate in the subtree is greater than in the single leaf, replace the whole subtree by a leaf node.\n",
    "    5. Return the pruned decision tree.\n",
    "    \"\"\"\n",
    "    \n",
    "    if(dt.tree[\"label\"]!=-1):\n",
    "        return dt\n",
    "    if(len(y)==0):\n",
    "        dt.tree[\"label\"]=0\n",
    "        return dt\n",
    "    \n",
    "    att=dt.tree[\"split_attribute\"]\n",
    "    sv=dt.tree[\"split_value\"]\n",
    "    X_left, X_right, y_left, y_right= partition_classes(X, y, att, sv)\n",
    "    pruning(dt.tree[\"leftTree\"],X_left,y_left)\n",
    "    pruning(dt.tree[\"rightTree\"],X_right,y_right)\n",
    "    tree_error= 1-DecisionTreeEvalution(dt,X,y)\n",
    "    leaf_error= DecisionTreeError(y)\n",
    "    if(tree_error>leaf_error):\n",
    "        dt.tree[\"label\"]=stats.mode(y)[0][0]\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should make use of the decision tree you trained in part1. Due the imbalance in our dataset, the post-pruning does not necessarily have better accuracy on test set. However, you will receive a free lunch after post-pruning. The free lunch is a more efficient algorithm. We will award full credits as long as your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function. You don't have to modify it.\n",
    "# pruning the full grown decision tree using validation set \n",
    "# dt should be a decision tree object that has been fully trained\n",
    "dt_pruned=pruning(dt, X_valid, y_valid)\n",
    "\n",
    "# Evaluate the decision tree using test set \n",
    "\n",
    "DecisionTreeEvalution(dt_pruned, X_test, y_test, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aGIzP6dtY-Y"
   },
   "source": [
    "# Part 4: Random Forests [35pts]\n",
    "\n",
    "The decision boundaries drawn by decision trees are very sharp, and fitting a decision tree of unbounded depth to a list of examples almost inevitably leads to **overfitting**. In an attempt to decrease the variance of our classifier we're going to use a technique called 'Bootstrap Aggregating' (often abbreviated 'bagging'). This stems from the idea that a collection of weak learners can learn decision boundaries as well as a strong learner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4.1 Random Forest Implementation (30pts)\n",
    "\n",
    "\n",
    "A Random Forest is a collection of decision trees, built as follows (**VERY IMPORTANT**):\n",
    "\n",
    "1) For every tree we're going to build:\n",
    "\n",
    "    a) Subsample the examples with replacement. Note that in this question, the size of the subsample data is equal to the original dataset. \n",
    "    \n",
    "    b) From the subsamples in a), choose attributes at random to learn on in accordance with a provided attribute subsampling rate. Based on what it was mentioned in the class, we randomly pick features in each split. We use a more general approach here to make the programming part easier. Let's randomly pick some features (70% percent of features) and grow the tree based on the pre-determined randomly selected features. Therefore, there is no need to find random features in each split.\n",
    "    \n",
    "    c) Fit a decision tree to the subsample of data we've chosen to a certain depth.\n",
    "    \n",
    "Classification for a random forest is then done by taking a majority vote of the classifications yielded by each tree in the forest after it classifies an example.\n",
    "\n",
    "In RandomForests Class, \n",
    "1. X is assumed to be a matrix with num_training rows and num_features columns where num_training is the\n",
    "number of total records and num_features is the number of features of each record. \n",
    "\n",
    "2. y is assumed to be a vector of labels of length num_training.\n",
    "\n",
    "**NOTE:** Lookout for TODOs for the parts that needs to be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6n8GGVU7tYGh"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE: For graduate student, you are required to use your own decision tree MyDecisionTree() to finish random forest.\n",
    "\"\"\"\n",
    "\n",
    "class RandomForest(object):\n",
    "    def __init__(self, n_estimators=50, max_depth=None, max_features=0.7):\n",
    "        # helper function. You don't have to modify it\n",
    "        # Initialization done here\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.bootstraps_row_indices = []\n",
    "        self.feature_indices = []\n",
    "        self.out_of_bag = []\n",
    "        self.decision_trees = [MyDecisionTree(max_depth=max_depth) for i in range(n_estimators)]\n",
    "        \n",
    "    def _bootstrapping(self, num_training, num_features):\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "        - Randomly select a sample dataset of size num_training with replacement from the original dataset. \n",
    "        - Randomly select certain number of features (num_features denotes the total number of features in X, \n",
    "          max_features denotes the percentage of features that are used to fit each decision tree) without replacement from the total number of features.\n",
    "        \n",
    "        Return:\n",
    "        - row_idx: the row indices corresponding to the row locations of the selected samples in the original dataset.\n",
    "        - col_idx: the column indices corresponding to the column locations of the selected features in the original feature list.\n",
    "        \n",
    "        Reference: https://en.wikipedia.org/wiki/Bootstrapping_(statistics)\n",
    "        \"\"\" \n",
    "        row_idx=np.random.randint(num_training, size=num_training)\n",
    "        fs=int(self.max_features*num_features)\n",
    "        col_idx=np.random.choice(num_features, fs, replace=False)\n",
    "        return row_idx, col_idx\n",
    "            \n",
    "    def bootstrapping(self, num_training, num_features):\n",
    "        # helper function. You don't have to modify it\n",
    "        # Initializing the bootstap datasets for each tree\n",
    "        for i in range(self.n_estimators):\n",
    "            total = set(list(range(num_training)))\n",
    "            row_idx, col_idx = self._bootstrapping(num_training, num_features)\n",
    "            total = total - set(row_idx)\n",
    "            self.bootstraps_row_indices.append(row_idx)\n",
    "            self.feature_indices.append(col_idx)\n",
    "            self.out_of_bag.append(total)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        Train decision trees using the bootstrapped datasets.\n",
    "        Note that you need to use the row indices and column indices.\n",
    "        \"\"\"\n",
    "        self.bootstrapping(len(y),X.shape[1])\n",
    "        for i in range(self.n_estimators):\n",
    "            row_idx=self.bootstraps_row_indices[i]\n",
    "            col_idx=self.feature_indices[i]\n",
    "            dt=self.decision_trees[i]\n",
    "            rX=X[row_idx]\n",
    "            new_X=rX[:,col_idx]\n",
    "            new_y=y[row_idx]\n",
    "            dt.fit(new_X,new_y,0)\n",
    "        \n",
    "    \n",
    "    def OOB_score(self, X, y):\n",
    "        # helper function. You don't have to modify it\n",
    "        accuracy = []\n",
    "        for i in range(len(X)):\n",
    "            predictions = []\n",
    "            for t in range(self.n_estimators):\n",
    "                if i in self.out_of_bag[t]:\n",
    "                    predictions.append(self.decision_trees[t].predict(X[i][self.feature_indices[t]]))\n",
    "            if len(predictions) > 0:\n",
    "                accuracy.append(np.sum(predictions == y[i]) / float(len(predictions)))\n",
    "        return np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4.2 Hyperparameter tuning(5pts)\n",
    "\n",
    "Change the hyperparamters below to obtain atleast a 83% accuracy of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "AC1-lWuct2wj",
    "outputId": "006bf714-6c0a-4d3e-f695-c1b326f0670d"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: \n",
    "n_estimators defines how many decision trees are fitted for the random forest (at least 10). \n",
    "max_depth defines a stop condition when the tree reaches to a certain depth.\n",
    "max_features controls the percentage of features that are used to fit each decision tree.\n",
    "Tune these three parameters to achieve a better accuracy (Required min. accuracy is 0.83.)\n",
    "The random forest fitting may take 5 - 15 minutes. We will not take running time into account when grading this part.\n",
    "\"\"\"\n",
    "n_estimators = 10\n",
    "max_depth = 5\n",
    "max_features = 1\n",
    "\n",
    "random_forest = RandomForest(n_estimators, max_depth, max_features)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "accuracy=random_forest.OOB_score(X_test, y_test)\n",
    "\n",
    "print(\"accuracy: %.4f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: SVM (35 Pts + 15 Pts Bonus)\n",
    "\n",
    "## 5.1 Fitting an SVM classifier by hand (20 Pts)\n",
    "\n",
    "Consider a dataset with 2 points in 1-dimensional space: $(x_1 = -2, y_1 = −1)$ and $(x_2 = 1, y_2 = 1)$. Here $x$ are the point coordinates and $y$ are the classes.\n",
    "\n",
    "Consider mapping each point to 3-dimensional space using the feature vector $\\phi(x) = [1,2x, x^2]$. (This is equivalent to using a second order polynomial kernel.) The max margin classifier has the form\n",
    "\n",
    "$$min ||\\mathbf{\\theta}||^2 s.t.$$\n",
    "\n",
    "$$y_1(\\phi(x_1)\\mathbf{\\theta} + b) ≥ 1 $$\n",
    "\n",
    "$$y_2(\\phi(x_2)\\mathbf{\\theta}+ b) ≥ 1 $$\n",
    "\n",
    "**Hint:** $\\phi(x_1)$ and $\\phi(x_2)$ are the suppport vectors. We have already given you the solution for the suppport vectors and you need to calculate back the parameters. Margin is equal to $\\frac{1}{||\\mathbf{\\theta}||}$ and full margin is equal to $\\frac{2}{||\\mathbf{\\theta}||}$.\n",
    "\n",
    "(1) Find a vector parallel to the optimal vector $\\mathbf{\\theta}$. (4pts)\n",
    "<br>\n",
    "<font color='red'>\n",
    "     $\\phi(x_1) = [1,-4, 4]$ and $\\phi(x_2) = [1,2, 1]$<br><br> \n",
    "     A vector parallel to the optimal vector would be $\\phi(x_2)-\\phi(x_1) = [1,2, 1]- [1,-4, 4] = [0,6,-3]$\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "(2) Calculate the value of the margin achieved by this $\\mathbf{\\theta}$? (4pts)\n",
    "<br>\n",
    "<font color='red'>\n",
    "     $margin = \\frac{\\sqrt{(0)^2+(6)^2+(-3)^2}}{2}=\\frac{\\sqrt{45}}{2}=\\frac{3\\sqrt{5}}{2}$\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "(3) Solve for $\\mathbf{\\theta}$, given that the margin is equal to $1/||\\mathbf{\\theta}||$. (4pts)\n",
    "<br>\n",
    "<font color='red'>\n",
    "     $\\theta=[0,6a,-3a]$ and $\\frac{1}{||\\theta||}=\\frac{3\\sqrt{5}}{2}$<br><br>\n",
    "     so $\\sqrt{(0)^2+(6a)^2+(-3a)^2}=\\frac{2}{3\\sqrt{5}}$,<br><br>\n",
    "     $3\\sqrt{5}a=\\frac{2}{3\\sqrt{5}}$,<br><br>\n",
    "     so a=$\\frac{2}{45}$<br><br>\n",
    "     and $\\theta=[0,\\frac{4}{15},-\\frac{2}{15}]$\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "(4) Solve for $b$ using your value for $\\mathbf{\\theta}$. (4pts)\n",
    "<br>\n",
    "<font color='red'>\n",
    "     $y_1(\\phi(x_1)\\theta+b)=1$<br><br>\n",
    "     $(1)([1,2,1]^T*[0,\\frac{4}{15},-\\frac{2}{15}]+b)=1$<br><br>\n",
    "     $\\frac{8}{15}-\\frac{2}{15}+b=1$<br><br>\n",
    "     $b=\\frac{9}{15}= \\frac{3}{5}$\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "(5) Write down the form of the discriminant function $f(x) = \\phi(x)\\mathbf{\\theta}+b$ as an explicit function of $x$. (4pts)\n",
    "<br>\n",
    "<font color='red'>\n",
    "     $f(x)=\\theta_0 + \\theta^T\\phi(x)$<br><br>\n",
    "     $f(x)=\\frac{3}{5} + [0,\\frac{4}{15},-\\frac{2}{15}]^T[1,2x,x^2]$<br><br>\n",
    "     $f(x)=\\frac{3}{5} + \\frac{8}{15}x-\\frac{2}{15}x^2$\n",
    "</font>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 SVM Kernel (15 Pts)\n",
    "\n",
    "(1) (5 points) We know that SVM can be used to classify linearly inseparable data by transforming it to a different feature space with a kernel $K(x, z) = \\phi(x)^T \\phi(z)$, where $\\phi(x)$ is a feature mapping ($x$ and $z$ are both data points). Let $K_1$ and $K_2$ be $R^n \\times R^n$ kernels, $c \\in R^+$ be a positive constant., and $\\phi_1,\\phi_2 : R^n → R^d$ be the respective feature mappings of $K_1$ and $K_2$. Explain how to use $\\phi_1,\\phi_2$ to obtain the following kernels:   \n",
    "a. $K(x, z) = cK_1(x, z)$\n",
    "<br>\n",
    "<font color='red'>\n",
    "     $K(x, z) = cK_1(x, z)$<br><br>\n",
    "     $\\phi(x)^T \\phi(z) = c\\phi_1(x)^T \\phi_1(z)$<br><br>\n",
    "     $\\phi(x)^T \\phi(z) = (\\sqrt{c}\\phi_1(x)^T)* (\\sqrt{c}\\phi_1(z))$<br><br>\n",
    "     $\\phi(x)= \\sqrt{c}\\phi_1(x)$\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "b. $K(x, z) = K_1(x, z)K_2(x, z)$\n",
    "<br>\n",
    "<font color='red'>\n",
    "     $K(x, z) = K_1(x, z)K_2(x, z)$<br><br>\n",
    "     $\\phi(x)^T \\phi(z) = (\\phi_1(x)^T \\phi_1(z))*(\\phi_2(x)^T \\phi_2(z))$<br><br>\n",
    "     $\\phi(x)^T \\phi(z) = (\\phi_1(x)^T \\phi_2(x)^T)*(\\phi_1(z) \\phi_2(z))$<br><br>\n",
    "     $\\phi(x)= \\phi_1(x) \\phi_2(x)$\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "(2) (10 points)  \n",
    "a. Consider the polynomial kernel $$K(x,z) = (x^T z + 1)^d$$ with d=2. Let $x,z \\in R$ for simplicity. Define one calculation as one multiplication, addition or square operation. Assume that constants (like $\\sqrt{2}$) are already calculated and given. What is the number of calculations required to find $K(x,z)$ through direct computation?\n",
    "<br>\n",
    "<font color='red'>\n",
    "    You would need <strong> 3 </strong> calculations. 1 multiplication $(x*z)$, 1 addition $(prev +1)$, 1 square $(prev)^2$.\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "b. Can you find the corresponding feature mapping $\\phi(x)$?\n",
    "<br>\n",
    "<font color='red'>\n",
    "    $K(x,z) = (x^T z + 1)^d$ with d=2<br><br>\n",
    "    $\\phi(x)^T \\phi(z) = (x^T)^2(z)^2 +2x^Tz+1$<br><br>\n",
    "    $\\phi(x)^T \\phi(z) = [(x^T)^2,x^T\\sqrt{2},1] * [z^2,z\\sqrt{2},1]$<br><br>\n",
    "    so, $\\phi(x)= [x^2,x\\sqrt{2},1]$\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "c. What is the number of calculations (count for different operations: addition, multiplication and square) required for calculating the above feature map for a scalar $x$ ?\n",
    "<br>\n",
    "<font color='red'>\n",
    "    The feature mapping would need <strong> 2 </strong> calculations. 1 square $(x^2)$ and 1 multiplication $(x*\\sqrt{2})$.\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "d. What is the number of calculations to find $K(x,z)$ using $\\phi(x)^T \\phi(z)$? Comment on this with respect to your answer in (a).\n",
    "<br>\n",
    "<font color='red'>\n",
    "    This would require <strong> 9 </strong> calculations. 1 square and 1 multiplication for feature mapping for both x and z(4), 1 multiplication for each of the 3 dimentions of the mapped feature (3), 2 additions that sum up the dot product addends (2). So overall 4+3+2= 9. This has 3 times more calculations required than part a, as it has 3 time the number of dimensions.\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "e. Consider the Radial Basis Kernel $$K(x,z) = exp ( - \\frac{||x - z||^2} {2\\sigma^2})$$. Is it possible to find a feature map in this case? Do you think it's necessary that an explicit feature map exists with all kernels?  \n",
    "<br>\n",
    "<font color='red'>\n",
    "    It would <strong>not be possible </strong> to derive a feature map for this case, as it would be infinite dimensional, even though an implicit feature map would exsist. <strong>It is not necessary that an explicit feature map exists </strong>because or program can compute an implicit feature mapping and work around that. \n",
    "</font>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 SVM Implementation (15 Pts - Bonus for all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please read the following instructions carefully and also read the descriptions given in function docstring.\n",
    "\n",
    "In this part, you will implement your own SVM classifier and do some feature engineering to work with non-linear decision boundaries.\n",
    "\n",
    "The \"reg\" parameter passed to the class is to be used with the hinge loss i.e. \n",
    "$$\n",
    "\\begin{equation}\n",
    "       \\text{hinge_loss} = \\text{reg} \\cdot \\frac{1}{N} \\sum_{i=1}^N \\max(0, 1 - y_i(x_i \\cdot \\theta))\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "And similarly, should also be included in the gradient used in update rule for $W$.\n",
    "\n",
    "The maximum number of epochs is set to 5000 but the model shouldn't take that many epochs to converge. You should check if the update changes cost by more than previous cost times the eps parameter and stop the iteration if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'svm_data_1.npz'\n",
    "# np.savez(filename, X, y)\n",
    "\n",
    "npz_data = np.load(filename)\n",
    "X = npz_data['arr_0']\n",
    "y = npz_data['arr_1']\n",
    "\n",
    "f, ax = plt.subplots(nrows=1, ncols=1,figsize=(5,5))\n",
    "\n",
    "plt.scatter(X[y==1,0],X[y==1,1], color='steelblue', label='1')\n",
    "plt.scatter(X[y==-1,0],X[y==-1,1], color='tomato', label='-1')\n",
    "plt.legend()\n",
    "ax.set_title(\"Synthetic Dataset\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(object):\n",
    "    def __init__(self, reg = 1000, lr = 1e-8, eps=0.01, print_every=20):\n",
    "        '''\n",
    "        You don't have to modify this function\n",
    "        Initialization done here\n",
    "        '''\n",
    "        \n",
    "        self.W = None\n",
    "        self.reg = reg\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.print_every = 20\n",
    "    \n",
    "    def compute_cost(self, X, y):\n",
    "        '''\n",
    "        TODO - Compute the total cost function for SVM\n",
    "        \n",
    "        returns:\n",
    "            cost - float, total value of cost function\n",
    "        '''\n",
    "        # calculate hinge loss\n",
    "        dist= 1- y * (np.dot(X, self.W))\n",
    "        dist[dist < 0] = 0\n",
    "        hl= self.reg * (np.sum(dist)/len(y))\n",
    "\n",
    "        # calculate cost\n",
    "        cost = 0.5 * np.dot(self.W, self.W) + hl\n",
    "        return cost\n",
    "    \n",
    "    def calculate_cost_gradient(self, X_point, y_point):\n",
    "        '''\n",
    "        TODO - Compute the total cost function for SVM\n",
    "        \n",
    "        returns:\n",
    "            dW - (D, ) array of piecewise gradient of SVM loss \n",
    "                 function\n",
    "        '''\n",
    "        dist= 1-y_point*np.dot(X_point, self.W)\n",
    "        dw= np.zeros(len(self.W))\n",
    "        if dist<=0:\n",
    "            dw = self.W\n",
    "        else:\n",
    "            dw = self.W -(self.reg * y_point * X_point)\n",
    "        return dw\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        '''\n",
    "        TODO - Use SGD to fit W to data using SVM update rule.\n",
    "        \n",
    "        You should use calculate_cost_gradient function\n",
    "        to get the gradient for update rule and compute_cost\n",
    "        function to get cost which can be displayed after \n",
    "        every few epochs. \n",
    "        \n",
    "        features doesn't include 1's for bias term. It \n",
    "        needs to be separately accounted for in your code.\n",
    "        \n",
    "        Use eps, print_every and lr defined in __init__()\n",
    "        \n",
    "        Update self.W directly.\n",
    "        \n",
    "        returns: None\n",
    "        '''\n",
    "        max_epochs= 5000\n",
    "        prev_cost= np.inf\n",
    "        oc= np.array([np.ones(X_train.shape[0])]).T\n",
    "        #print(oc.shape)\n",
    "        #print(X_train.shape)\n",
    "        X_train= np.hstack((oc,X_train))\n",
    "        self.W= np.zeros(X_train.shape[1])\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            for i in range(len(y_train)):\n",
    "                dw= self.calculate_cost_gradient(X_train[i], y_train[i])\n",
    "                self.W = self.W-(self.lr * dw)\n",
    "            if (epoch%self.print_every==0  or epoch==max_epochs-1):\n",
    "                cost= self.compute_cost(X_train, y_train)\n",
    "                print(\"Epoch is:{} and Cost is: {}\".format(epoch, cost))\n",
    "                if abs(prev_cost - cost) < self.eps * prev_cost:\n",
    "                    return self.W\n",
    "                prev_cost = cost\n",
    "        return self.W\n",
    "\n",
    "        \n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        TODO - Predict y_test from W updated in fit function.\n",
    "        \n",
    "        X_test doesn't include 1's for bias term. It \n",
    "        needs to be separately accounted for in your code.\n",
    "        \n",
    "        returns: \n",
    "            y_test_predicted: (N, ) array predicted by model \n",
    "                              with values from {-1, 1} \n",
    "        '''\n",
    "        #  Delete this line when you implement the function\n",
    "        oc= np.array([np.ones(X_test.shape[0])]).T\n",
    "        X_test= np.hstack((oc,X_test))\n",
    "        y_test_predicted=np.sign(np.dot(X_test,self.W))\n",
    "        return y_test_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "# Train the SVM classifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "svm_cls = SVM()\n",
    "svm_cls.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "# Check test accuracy\n",
    "# You should get more than 90% accuracy\n",
    "\n",
    "y_test_predicted = svm_cls.predict(X_test)\n",
    "\n",
    "print(\"Accuracy on test dataset: {}\".format(accuracy_score(y_test, \n",
    "                                                           y_test_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_decision_boundary(X, y, feature_new=None, h=0.02):\n",
    "    '''\n",
    "    You don't have to modify this function\n",
    "    \n",
    "    Function to vizualize decision boundary\n",
    "    \n",
    "    feature_new is a function to get X with additional features\n",
    "    '''\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx_1, xx_2 = np.meshgrid(np.arange(x1_min, x1_max, h),\n",
    "                         np.arange(x2_min, x2_max, h))\n",
    "\n",
    "    if X.shape[1] == 2:\n",
    "        Z = svm_cls.predict(np.c_[xx_1.ravel(), xx_2.ravel()])\n",
    "    else:\n",
    "        X_conc = np.c_[xx_1.ravel(), xx_2.ravel()]\n",
    "        X_new = feature_new(X_conc)\n",
    "        Z = svm_cls.predict(X_new)\n",
    "\n",
    "    Z = Z.reshape(xx_1.shape)\n",
    "    \n",
    "    f, ax = plt.subplots(nrows=1, ncols=1,figsize=(5,5))\n",
    "    plt.contourf(xx_1, xx_2, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel('X_1')\n",
    "    plt.ylabel('X_2')\n",
    "    plt.xlim(xx_1.min(), xx_1.max())\n",
    "    plt.ylim(xx_2.min(), xx_2.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "# Visualize decision boundary\n",
    "\n",
    "visualize_decision_boundary(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Feature Mapping (5 Pts - Bonus for all)\n",
    "\n",
    "As we can see above, our model is able to achieve a good acuracy on dataset whose most datapoints are linearly seperable. Let's see another dataset where the datapoint can't be classified with a good accuracy using a linear classifier. Run the below cell to generate the dataset.\n",
    "\n",
    "We will also see what happens when we try to fit a linear classifier to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "# Generate dataset\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "X, y = make_circles(n_samples = 1000, \n",
    "                    noise = 0.02, \n",
    "                    random_state=random_state, \n",
    "                    factor=0.6) \n",
    "\n",
    "y = np.where(y == 0, -1, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=random_state)\n",
    "\n",
    "f, ax = plt.subplots(nrows=1, ncols=1,figsize=(5,5))\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y, marker = '.') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "# Try to fit a linear classifier to the dataset\n",
    "\n",
    "svm_cls = SVM(lr=1e-7)\n",
    "svm_cls.fit(X_train, y_train)\n",
    "y_test_predicted = svm_cls.predict(X_test)\n",
    "\n",
    "print(\"Accuracy on test dataset: {}\".format(accuracy_score(y_test, \n",
    "                                                           y_test_predicted)))\n",
    "\n",
    "visualize_decision_boundary(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we need a non-linear boundary to be able to successfully classify data in this dataset. In the function below add additional features which can help classify in the above dataset. After creating the additional features use code in the further cells to see how well the features perform on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nl_feature(X):\n",
    "    '''\n",
    "    TODO - Create additional features and add it to the dataset\n",
    "    \n",
    "    returns:\n",
    "        X_new - (N, d + num_new_features) array with \n",
    "                additional features added to X such that it\n",
    "                can classify the points in the dataset.\n",
    "    '''\n",
    "    #  Delete this line when you implement the function\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "# Create new features\n",
    "\n",
    "X_new = create_nl_feature(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "# Fit to the new features and vizualize the decision boundary\n",
    "# You should get more than 90% accuracy on test set\n",
    "\n",
    "svm_cls = SVM(lr=1e-7)\n",
    "svm_cls.fit(X_train, y_train)\n",
    "y_test_predicted = svm_cls.predict(X_test)\n",
    "\n",
    "print(\"Accuracy on test dataset: {}\".format(accuracy_score(y_test, y_test_predicted)))\n",
    "\n",
    "visualize_decision_boundary(X_train, y_train, create_nl_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6 Naive Bayes and Logistic Regression (30 Pts)\n",
    "## 6.1 Naive Bayes (20 points)\n",
    "Suppose we want to go to a golf court. However, the golf court is only open in certain weathers. Here you have the data for the last 10 days:\n",
    "\n",
    "\n",
    "| Outlook  | Temperature | Windy  | Court  |\n",
    "|---|---|---|---|\n",
    "| Rainy | 50 | False | Closed |\n",
    "| Rainy | 30 | True | Closed |\n",
    "| Overcast | 20 | False | Open |\n",
    "| Sunny | 10 | False | Open |\n",
    "| Sunny | 30 | False | Open |\n",
    "| Overcast | 50 | True | Closed |\n",
    "| Sunny | 20 | True | Open |\n",
    "| Rainy | 40 | False | Closed |\n",
    "| Sunny | 40 | True | Closed |\n",
    "| Rainy | 10 | False | Open |\n",
    "\n",
    "\n",
    "(1) (3 points) Estimate the parameters for each conditional distribution P($X_i$|Y ) using maximum likelihood estimation, where $X_i$ is the individual feature, and Y is the class label. \n",
    "\n",
    "For example, assume P(Outlook|Y ) follows a multinomial distribution, then for Y = Open:  \n",
    "P(Rainy | Open) = 1/5   \n",
    "P(Overcast | Open) = 1/5  \n",
    "P(Sunny | Open) = 3/5  \n",
    "\n",
    "Estimate P(Outlook|Y ) for Y = Closed, where P(Outlook|Y ) is a multinomial distribution.\n",
    "<br>\n",
    "<font color='red'>\n",
    "    P(Rainy|Closed)=3/5 <br>\n",
    "    P(Overcast|Closed)=1/5 <br>\n",
    "    P(Sunny|CLosed)=1/5\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "(2) (4 points) Assume P(Windy|Y ) is a Bernoulli distribution with parameter $\\sigma_Y$ , (i.e., P(Windy=True|Y ) = $\\sigma_Y$ and P(Windy=False|Y ) = $1 − \\sigma_Y$ ), please estimate $\\sigma_Y$ for both Y = Open and Y = Closed.\n",
    "<br>\n",
    "<font color='red'>\n",
    "    P(Windy=True|Open)=1/5 <br>\n",
    "    P(Windy=False|Open)=4/5 <br>\n",
    "    $\\sigma_O= 1/5$ <br>\n",
    "    <br>\n",
    "    P(Windy=True|Closed)=3/5 <br>\n",
    "    P(Windy=False|Closed)=2/5 <br>\n",
    "    $\\sigma_C= 3/5$ \n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "(3) (5 points) Assume P(Temperature|Y ) is a uniform distribution in the range of $[l_Y , h_Y ]$, thus the\n",
    "density P(Temperature|Y ) = $\\frac{1}{h_Y −l_Y}$.\n",
    "Please estimate $[l_Y , h_Y ]$ for both Y = Open and Y = Closed.\n",
    "<br>\n",
    "<font color='red'>\n",
    "    P(Temperature $ \\in [10 , 30 ]$ |Open)=1/(30-10)= 1/20 <br>\n",
    "    P(Temperature $ \\notin [10 , 30 ]$ |Open)=0 <br><br>\n",
    "    P(Temperature $ \\in [30 , 50 ]$ |Closed)=1/(50-30)= 1/20 <br>\n",
    "    P(Temperature $ \\notin [30 , 50 ]$ |Closed)=0 \n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "(4) (8 points) Use Naive Bayes method to estimate if golf court will be open on a day with parameters (Overcast, 30, False).\n",
    "<br>\n",
    "<font color='red'>\n",
    "    We have to determine which is greater between P(Open|Overcast, 30, False) and P(Closed|Overcast, 30, False). <br><r>\n",
    "    Using bayes rule, P(Open|Overcast, 30, False) = $\\frac{P(Overcast, 30, False|Open)P(Open)}{P(Overcast, 30, False)}$\n",
    "    and P(Closed|Overcast, 30, False) = $\\frac{P(Overcast, 30, False|Closed)P(Closed)}{P(Overcast, 30, False)}$ <br><br>\n",
    "    assuming independence among features, and canceling the denominators (since the are the same), the court will be open if: <br><br> $P(Overcast|Open)P(30|Open)P(False|Open)P(Open) > P(Overcast|Closed)P(30|Closed)P(False|Closed)P(Closed)$ <br><br> $P(Overcast|Open)P(30|Open)P(False|Open)P(Open)= \\frac{1}{5} * \\frac{1}{20} * \\frac{4}{5} * \\frac{1}{2}=  \\frac{4}{2500}$ <br><br>\n",
    " <br> $P(Overcast|Open)P(30|Open)P(False|Open)P(Open)= \\frac{1}{5} * \\frac{1}{20} * \\frac{2}{5} * \\frac{1}{2}=  \\frac{2}{2500}$ <br><br> \n",
    "    since $\\frac{2}{2500} > \\frac{2}{2500}$, <strong>the court will be open</strong>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Logistic Regression (10 points)\n",
    "(1) (2 points) Is Logistic Regression a discriminative or generative model? Why?\n",
    "<br>\n",
    "<font color='red'>\n",
    "    Logistic Regression is a discriminative model as it directly esimates the decision boundary/ posterior distribution from the input without any assumption about the independence of features.\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "(2) (3 points) Can you apply Logisitic Regression for more than two classes? How?\n",
    "<br>\n",
    "<font color='red'>\n",
    "    Yes, you can apply Logisitic Regression for more than two classes. There are multiple ways of doing so. One way would be to use the softmax function instead of the sigmoid function for the cross entropy loss. Another way to do so would be to train a logistic regression model for each of the classes, and select the class that has the highest probability.\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "(3) (5 points) Will Logisitic Regression work well for the following sets of data? Why or why not?  \n",
    "Data 1:  \n",
    "$(x_1 = 3, x_2 = 1, y =1), (x_1= 0, x_2 = 0, y =-1), (x_1 = -1, x_2 = -3, y = 1), (x_1 = 1, x_2 = 1, y = -1), (x_1 = 2, x_2 = 0, y = 1), (x_1 = -2, x_2 = 1, y = -1)$  \n",
    "Data 2:  \n",
    "$(x_1 = 2, x_2 = 1, y =1), (x_1= 0, x_2 = 0, y =-1), (x_1 = -1, x_2 = -3, y = 1), (x_1 = 1, x_2 = 1, y = 1), (x_1 = 2, x_2 = 0, y = -1), (x_1 = -2, x_2 = 1, y = -1)$\n",
    "<br>\n",
    "<font color='red'>\n",
    "    Logistic regression will work well with Data 1, as the points have a linear decision boundary. Logistic regression will not work well with Data 2, as it seems to have a non linear decsion boundary.\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW3_solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
